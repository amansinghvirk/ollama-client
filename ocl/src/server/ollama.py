from ollama import Client
from src.utilities.config import get_hostname


def save_hostname(ollama_server_path):
    """
    Save the Ollama server hostname to a .env file.

    Args:
        ollama_server_path (str): The hostname of the Ollama server.

    Returns:
        bool: True if the hostname is saved successfully, False otherwise.

    Raises:
        None

    """
    if ollama_client(hostserver=ollama_server_path) is None:
        return False

    with open(".env", "w") as f:
        f.write(f"OLLAMA_SERVER={ollama_server_path}\n")

    return True


def validate_connection(client):
    """
    Validate the connection to the Ollama server.

    Args:
        client (ollama.Client): The Ollama client object.

    Returns:
        bool: True if the connection is successful, False otherwise.

    Raises:
        None

    """
    try:
        client.list()
        return True
    except Exception as e:
        return False


def ollama_client(hostserver=None):
    """
    Create an instance of the Ollama client.

    Args:
        hostserver (str, optional): The hostname of the Ollama server. If not provided, it will be fetched from the configuration.

    Returns:
        ollama.Client: The Ollama client object if the connection is successful, None otherwise.

    Raises:
        None

    """
    if hostserver is None:
        hostserver = get_hostname()
        if hostserver is None:
            return None

    try:
        client = Client(host=hostserver)
        if validate_connection(client):
            return client
        return None
    except Exception as e:
        print(e)
        return None


def get_llm_system_response(model):
    """
    Get the response from the Ollama server for the given model.

    Args:
        model (str): The name of the model to use for generating the response.

    Returns:
        str: The response generated by the Ollama server.

    Raises:
        None

    """
    client = ollama_client()
    if client:
        return client.generate(model=model, prompt="Hi").get("response")
    return "Server connection not established"


def get_llm_response(model, user_message):
    """
    Get the response from the Ollama server for the given user message.

    Args:
        model (str): The name of the model to use for generating the response.
        user_message (str): The message provided by the user.

    Returns:
        str: The response generated by the Ollama server.

    Raises:
        None

    """
    client = ollama_client()
    if client:
        response = client.chat(
            model=model,
            messages=[
                {
                    "role": "user",
                    "content": user_message,
                },
            ],
        )

        return response["message"]["content"]
    return "Server connection not established"


def get_ollama_models():
    """
    Get the list of available Ollama models.

    Args:
        None

    Returns:
        list: A list of model names.

    Raises:
        None

    """
    client = ollama_client()

    models = []
    if client:
        for model in client.list().get("models"):
            models.append(model.get("name"))

    return models


def pull_ollama_model(model):
    """
    Pull the specified Ollama model from the server.

    Args:
        model (str): The name of the model to pull.

    Returns:
        bool: True if the model is pulled successfully, False otherwise.

    Raises:
        None

    """
    client = ollama_client()
    if client is None:
        False

    try:
        client.pull(model)
        return True
    except Exception as e:
        print(e)
        return False


def delete_ollama_model(model):
    """
    Delete the specified Ollama model from the server.

    Args:
        model (str): The name of the model to delete.

    Returns:
        bool: True if the model is deleted successfully, False otherwise.

    Raises:
        None

    """
    client = ollama_client()
    if client is None:
        False
    try:
        client.delete(model)
        return True
    except Exception as e:
        print(e)
        return False


def create_ollama_model(model_name, modelfile):
    """
    Create a new Ollama model on the server.

    Args:
        model_name (str): The name of the model to create.
        modelfile (str): The path to the model file.

    Returns:
        bool: True if the model is created successfully, False otherwise.

    Raises:
        None

    """
    client = ollama_client()
    if client is None:
        return False

    try:
        client.create(model=model_name, modelfile=modelfile)
        return True
    except Exception as e:
        print(e)
        return False
