from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder


from langchain_core.messages import HumanMessage
from langchain.memory import ChatMessageHistory



from src.server.ollama import ollama_client
from src.utilities.config import get_hostname



def get_langchain_llm_response(model, user_message, memory_data):
    """
    Retrieves a response from the language model using the provided user message.

    Args:
        model (str): The name of the language model.
        user_message (str): The user's message.

    Returns:
        str: The response generated by the language model.

    Raises:
        None

    """
    client = ollama_client()
    if client:
        hostserver = get_hostname()
        llm = ChatOllama(model=model, base_url=hostserver)

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a helpful assistant. Answer all questions to the best of your ability.",
                ),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )

        response = ""
        if user_message.strip() != "":

            chat_history = ChatMessageHistory()
            
            for messages in memory_data["chat-history"]:
                chat_history.add_user_message(messages["User"])
                chat_history.add_ai_message(messages["Agent"])


            chat_history.add_user_message(user_message)
            chain = prompt | llm

            ai_response = chain.invoke({'messages': chat_history.messages})
            response = ai_response.content

            chat_history.add_ai_message(response)

            memory_data["chat-history"].append({"User": user_message, "Agent": response})

        return memory_data

    return "Server connection not established"


def get_session_chat_title(model, memory_data):
    """
    Retrieves a response from the language model using the provided user message.

    Args:
        model (str): The name of the language model.
        user_message (str): The user's message.

    Returns:
        str: The response generated by the language model.

    Raises:
        None

    """
    client = ollama_client()
    if client:
        hostserver = get_hostname()
        llm = ChatOllama(model=model, base_url=hostserver)

        user_message = """
            Provide 3-4 word title to the conversation history. 
            Response should only contain title and no explanation is required
        """

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a assistant. Expert in providing 3-4 words title to long conversations",
                ),
                MessagesPlaceholder(variable_name="messages"),
            ]
        )

        response = ""
        if user_message.strip() != "":

            chat_history = ChatMessageHistory()
            
            for messages in memory_data["chat-history"]:
                chat_history.add_user_message(messages["User"])
                chat_history.add_ai_message(messages["Agent"])


            chat_history.add_user_message(user_message)
            chain = prompt | llm

            ai_response = chain.invoke({'messages': chat_history.messages})
            response = ai_response.content

        return response

    return "Server connection not established"
